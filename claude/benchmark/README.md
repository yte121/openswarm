# Agent Swarm Benchmarking Tool

A comprehensive Python-based benchmarking tool for agent swarms that interfaces with the Claude Flow Advanced Swarm System. This tool measures performance, efficiency, and effectiveness of different swarm strategies and coordination modes.

## ðŸš€ Features

- **Complete Strategy Coverage**: Supports all claude-flow swarm strategies (auto, research, development, analysis, testing, optimization, maintenance)
- **Multiple Coordination Modes**: Tests centralized, distributed, hierarchical, mesh, and hybrid coordination patterns
- **Comprehensive Metrics**: Tracks performance, resource usage, quality metrics, and coordination efficiency
- **Multiple Output Formats**: Exports results to JSON, SQLite, CSV, and HTML formats
- **CLI Interface**: Command-line interface matching claude-flow swarm command structure
- **Test-Driven Development**: Built using TDD methodology with comprehensive test coverage
- **Modular Architecture**: Clean, extensible design with pluggable components

## ðŸ“¦ Installation

### Prerequisites
- Python 3.8 or higher
- pip package manager

### Install from Source
```bash
# Clone and install
cd benchmark
pip install -r requirements.txt
pip install -e .
```

## ðŸŽ¯ Quick Start

### Basic Usage
```bash
# Run a simple research benchmark
swarm-benchmark run "Research cloud architecture patterns" --strategy research

# Run a development benchmark with distributed coordination
swarm-benchmark run "Build a REST API" --strategy development --mode distributed

# Run an analysis task with monitoring
swarm-benchmark run "Analyze user behavior data" --strategy analysis --monitor

# Test different strategies automatically
swarm-benchmark run "Optimize database performance" --strategy auto
```

### Example Output
```bash
$ swarm-benchmark run "Test research task" --strategy research --verbose

Running benchmark: benchmark-research-centralized
Objective: Test research task
Strategy: research
Mode: centralized
âœ… Benchmark completed successfully!
ðŸ“Š Results saved to: ./reports
ðŸ“‹ Summary: Completed 1 tasks
```

## ðŸ“Š Available Strategies

| Strategy | Description | Best For |
|----------|-------------|----------|
| `auto` | Automatically determines best approach | General-purpose tasks |
| `research` | Information gathering and analysis | Research, documentation |
| `development` | Software development and coding | Building applications |
| `analysis` | Data analysis and insights | Data processing, metrics |
| `testing` | Quality assurance workflows | Testing, validation |
| `optimization` | Performance improvements | Speed, efficiency gains |
| `maintenance` | System maintenance tasks | Updates, documentation |

## ðŸ”— Coordination Modes

| Mode | Description | Coordination Pattern |
|------|-------------|---------------------|
| `centralized` | Single coordinator (default) | Simple, reliable |
| `distributed` | Multiple coordinators | Scalable, fault-tolerant |
| `hierarchical` | Tree structure | Organized, clear authority |
| `mesh` | Peer-to-peer | Flexible, dynamic |
| `hybrid` | Mixed patterns | Adaptive, optimized |

## Development

This project follows the SPARC (Specification, Pseudocode, Architecture, Refinement, Completion) methodology for systematic Test-Driven Development.

### Running Tests

```bash
pytest tests/
```

### Project Structure

```
benchmark/
â”œâ”€â”€ src/swarm_benchmark/    # Source code
â”‚   â”œâ”€â”€ cli/               # Command-line interface
â”‚   â”œâ”€â”€ core/             # Core benchmarking framework
â”‚   â”œâ”€â”€ strategies/       # Swarm strategy implementations
â”‚   â”œâ”€â”€ modes/           # Coordination mode implementations
â”‚   â”œâ”€â”€ metrics/         # Performance metrics collection
â”‚   â”œâ”€â”€ output/          # JSON/SQLite output modules
â”‚   â””â”€â”€ utils/           # Utility functions
â”œâ”€â”€ tests/               # Test suite
â”‚   â”œâ”€â”€ unit/           # Unit tests
â”‚   â”œâ”€â”€ integration/    # Integration tests
â”‚   â””â”€â”€ performance/    # Performance benchmarks
â””â”€â”€ config/             # Configuration files
```

## License

MIT License